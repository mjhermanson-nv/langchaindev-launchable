import marimo

__generated_with = "0.17.8"
app = marimo.App()


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    # NVIDIA NIMs

    The `langchain-nvidia-ai-endpoints` package contains LangChain integrations building applications with models on
    NVIDIA NIM inference microservice. NIM supports models across domains like chat, embedding, and re-ranking models
    from the community as well as NVIDIA. These models are optimized by NVIDIA to deliver the best performance on NVIDIA
    accelerated infrastructure and deployed as a NIM, an easy-to-use, prebuilt containers that deploy anywhere using a single
    command on NVIDIA accelerated infrastructure.

    NVIDIA hosted deployments of NIMs are available to test on the [NVIDIA API catalog](https://build.nvidia.com/). After testing,
    NIMs can be exported from NVIDIA’s API catalog using the NVIDIA AI Enterprise license and run on-premises or in the cloud,
    giving enterprises ownership and full control of their IP and AI application.

    NIMs are packaged as container images on a per model basis and are distributed as NGC container images through the NVIDIA NGC Catalog.
    At their core, NIMs provide easy, consistent, and familiar APIs for running inference on an AI model.
    """)
    return


@app.cell
def _():
    # '%pip install -U langchain-nvidia-ai-endpoints langchain-community langchain langgraph tavily-python beautifulsoup4 lxml' command supported automatically in marimo
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    # NVIDIA NIM RAG agent with LLaMA3

    We'll combine ideas from paper RAG papers into a RAG agent:

    - **Routing:**  Adaptive RAG ([paper](https://arxiv.org/abs/2403.14403)). Route questions to different retrieval approaches
    - **Fallback:** Corrective RAG ([paper](https://arxiv.org/pdf/2401.15884.pdf)). Fallback to web search if docs are not relevant to query
    - **Self-correction:** Self-RAG ([paper](https://arxiv.org/abs/2310.11511)). Fix answers w/ hallucinations or don’t address question

    ![langgraph_adaptive_rag.png](attachment:7b00797e-fb85-4474-9a9e-c505b61add81.png)

    ## NVIDIA NIM Models
    - [NVIDIAEmbeddings](https://python.langchain.com/v0.1/docs/integrations/text_embedding/nvidia_ai_endpoints/) using [nv-embed-qa](https://build.nvidia.com/nvidia/embed-qa-4)
    - [ChatNVIDIA](https://python.langchain.com/v0.1/docs/integrations/chat/nvidia_ai_endpoints/) using [llama3](https://build.nvidia.com/meta/llama3-70b)
    """)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ### Tracing

    ```python
    import getpass
    import os

    os.environ['LANGCHAIN_TRACING_V2'] = 'true'
    os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'
    os.environ['LANGCHAIN_API_KEY'] = '<your-api-key>'
    ```
    """)
    return


@app.cell
def _():
    import getpass
    import os

    if not os.environ.get("NVIDIA_API_KEY", "").startswith("nvapi-"):
        nvapi_key = getpass.getpass("Enter your NVIDIA API key: ")
        assert nvapi_key.startswith("nvapi-"), f"{nvapi_key[:5]}... is not a valid key"
        os.environ["NVIDIA_API_KEY"] = nvapi_key
    return getpass, os


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ### Search

    We'll use [Tavily](https://tavily.com/) for web search.
    """)
    return


@app.cell
def _(getpass, os):
    if not os.environ.get("TAVILY_API_KEY", ""):
        tyapi_key = getpass.getpass("Enter your Tavily API key: ")
        os.environ["TAVILY_API_KEY"] = tyapi_key
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ### Model

    First, we can select a model that supports tool calling.
    """)
    return


@app.cell
def _():
    from langchain_nvidia_ai_endpoints import ChatNVIDIA
    tool_models = [model for model in ChatNVIDIA.get_available_models() if model.supports_tools]
    tool_models
    return (ChatNVIDIA,)


@app.cell
def _(ChatNVIDIA):
    # Select a model 
    model_id = "meta/llama-3.1-70b-instruct"
    model_id = "meta/llama-3.1-8b-instruct"
    llm = ChatNVIDIA(model=model_id, temperature=0)
    return (llm,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ### Index

    Define an index that we want to use.
    """)
    return


@app.cell
def _():
    ### Index
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.document_loaders import WebBaseLoader
    from langchain_core.vectorstores import InMemoryVectorStore
    from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings
    urls = ['https://lilianweng.github.io/posts/2023-06-23-agent/', 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/']
    _docs = [WebBaseLoader(url).load() for url in urls]
    docs_list = [item for sublist in _docs for item in sublist]
    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=250, chunk_overlap=0)
    doc_splits = text_splitter.split_documents(docs_list)
    vectorstore = InMemoryVectorStore.from_documents(documents=doc_splits, embedding=NVIDIAEmbeddings(model='NV-Embed-QA'))
    # Add to vectorDB
    # Create retriever
    retriever = vectorstore.as_retriever(k=3)
    return (retriever,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ### Components

    Test structured output across our components.
    """)
    return


@app.cell
def _(llm):
    ### Router

    import json
    from typing_extensions import TypedDict, List, Annotated, Literal
    from pydantic import BaseModel, Field
    from langchain_core.messages import HumanMessage, SystemMessage

    # Data model
    class RouteQuery(BaseModel):
        """Route a user query to the most relevant datasource."""

        datasource: Literal["vectorstore", "websearch"] = Field(
            ...,
            description="Given a user question choose to route it to web search or a vectorstore.",
        )

    # LLM with structured output
    structured_llm_router = llm.with_structured_output(RouteQuery)

    # Prompt
    router_instructions = """You are an expert at routing a user question to a vectorstore or web search.

    The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.

    Use the vectorstore for questions on these topics. For all else, and especially for current events, use web-search.

    Return structured output with single key, datasource, that is 'websearch' or 'vectorstore' depending on the question."""

    # Test router
    test_web_search = structured_llm_router.invoke(
        [SystemMessage(content=router_instructions)]
        + [
            HumanMessage(
                content="Who is favored to win the NFC Championship game in the 2024 season?"
            )
        ]
    )
    test_web_search_2 = structured_llm_router.invoke(
        [SystemMessage(content=router_instructions)]
        + [HumanMessage(content="What are the models released today for llama3.2?")]
    )
    test_vector_store = structured_llm_router.invoke(
        [SystemMessage(content=router_instructions)]
        + [HumanMessage(content="What are the types of agent memory?")]
    )
    print(
        test_web_search,
        test_web_search_2,
        test_vector_store,
    )
    return (
        Annotated,
        BaseModel,
        Field,
        HumanMessage,
        List,
        RouteQuery,
        SystemMessage,
        TypedDict,
        router_instructions,
    )


@app.cell
def _(BaseModel, Field, HumanMessage, SystemMessage, llm, retriever):
    ### Retrieval Grader
    class GradeDocuments(BaseModel):
    # Data model
        """Binary score for relevance check on retrieved documents."""
        binary_score: str = Field(description="Documents are relevant to the question, 'yes' or 'no'")
    _structured_llm_grader = llm.with_structured_output(GradeDocuments)
    doc_grader_instructions = 'You are a grader assessing relevance of a retrieved document to a user question.\n\nIf the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.'
    doc_grader_prompt = "Here is the retrieved document: \n\n {document} \n\n Here is the user question: \n\n {question}. \n\nThis carefully and objectively assess whether the document contains at least some information that is relevant to the question.\n\nReturn structured output with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question."
    question = 'What is Chain of thought prompting?'
    _docs = retriever.invoke(question)
    # LLM with structured output
    doc_txt = _docs[1].page_content
    doc_grader_prompt_formatted = doc_grader_prompt.format(document=doc_txt, question=question)
    # Doc grader instructions
    _result = _structured_llm_grader.invoke([SystemMessage(content=doc_grader_instructions)] + [HumanMessage(content=doc_grader_prompt_formatted)])
    # Grader prompt
    # Test
    _result
    return GradeDocuments, doc_grader_instructions, doc_grader_prompt, question


@app.cell
def _(HumanMessage, llm, question, retriever):
    ### Generate
    rag_prompt = 'You are an assistant for question-answering tasks. \n\nHere is the context to use to answer the question:\n\n{context} \n\nThink carefully about the above context. \n\nNow, review the user question:\n\n{question}\n\nProvide an answer to this questions using only the above context. \n\nUse three sentences maximum and keep the answer concise.\n\nAnswer:'
    # Prompt

    def format_docs(docs):
        return '\n\n'.join((doc.page_content for doc in _docs))
    _docs = retriever.invoke(question)
    docs_txt = format_docs(_docs)
    rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)
    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])
    # Post-processing
    # Test
    print(generation.content)
    return docs_txt, format_docs, generation, rag_prompt


@app.cell
def _(
    BaseModel,
    Field,
    GradeDocuments,
    HumanMessage,
    SystemMessage,
    docs_txt,
    generation,
    llm,
):
    ### Hallucination Grader
    class GradeHallucinations(BaseModel):
    # Data model
        """Binary score for hallucination present in generation answer."""
        binary_score: str = Field(description="Answer is grounded in the facts, 'yes' or 'no'")
    _structured_llm_grader = llm.with_structured_output(GradeDocuments)
    hallucination_grader_instructions = '\n\nYou are a teacher grading a quiz. \n\nYou will be given FACTS and a STUDENT ANSWER. \n\nHere is the grade criteria to follow:\n\n(1) Ensure the STUDENT ANSWER is grounded in the FACTS. \n\n(2) Ensure the STUDENT ANSWER does not contain "hallucinated" information outside the scope of the FACTS.\n\nScore:\n\nA score of yes means that the student\'s answer meets all of the criteria. This is the highest (best) score. \n\nA score of no means that the student\'s answer does not meet all of the criteria. This is the lowest possible score you can give.\n\nExplain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n\nAvoid simply stating the correct answer at the outset.'
    hallucination_grader_prompt = "FACTS: \n\n {documents} \n\n STUDENT ANSWER: {generation}. \n\nReturn structured output with two two keys, binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER is grounded in the FACTS."
    hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(documents=docs_txt, generation=generation.content)
    _result = _structured_llm_grader.invoke([SystemMessage(content=hallucination_grader_instructions)] + [HumanMessage(content=hallucination_grader_prompt_formatted)])
    # LLM with structured output
    # Hallucination grader instructions
    # Grader prompt
    # Test using documents and generation from above
    _result
    return (
        GradeHallucinations,
        hallucination_grader_instructions,
        hallucination_grader_prompt,
    )


@app.cell
def _(BaseModel, Field, HumanMessage, SystemMessage, llm):
    class GradeAnswer(BaseModel):
        """Binary score to assess answer addresses question."""
        binary_score: str = Field(description="Answer addresses the question, 'yes' or 'no'")
    _structured_llm_grader = llm.with_structured_output(GradeAnswer)
    answer_grader_instructions = "You are a teacher grading a quiz. \n\nYou will be given a QUESTION and a STUDENT ANSWER. \n\nHere is the grade criteria to follow:\n\n(1) The STUDENT ANSWER helps to answer the QUESTION\n\nScore:\n\nA score of yes means that the student's answer meets all of the criteria. This is the highest (best) score. \n\nThe student can receive a score of yes if the answer contains extra information that is not explicitly asked for in the question.\n\nA score of no means that the student's answer does not meet all of the criteria. This is the lowest possible score you can give.\n\nExplain your reasoning in a step-by-step manner to ensure your reasoning and conclusion are correct. \n\nAvoid simply stating the correct answer at the outset."
    answer_grader_prompt = "QUESTION: \n\n {question} \n\n STUDENT ANSWER: {generation}. \n\nReturn structured output with binary_score is 'yes' or 'no' score to indicate whether the STUDENT ANSWER meets the criteria."
    question_1 = 'What are the vision models released today as part of Llama 3.2?'
    answer = "The Llama 3.2 models released today include two vision models: Llama 3.2 11B Vision Instruct and Llama 3.2 90B Vision Instruct, which are available on Azure AI Model Catalog via managed compute. These models are part of Meta's first foray into multimodal AI and rival closed models like Anthropic's Claude 3 Haiku and OpenAI's GPT-4o mini in visual reasoning. They replace the older text-only Llama 3.1 models."
    answer_grader_prompt_formatted = answer_grader_prompt.format(question=question_1, generation=answer)
    _result = _structured_llm_grader.invoke([SystemMessage(content=answer_grader_instructions)] + [HumanMessage(content=answer_grader_prompt_formatted)])
    _result
    return GradeAnswer, answer_grader_instructions, answer_grader_prompt


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ### Graph
    """)
    return


@app.cell
def _():
    ### Search
    from langchain_community.tools.tavily_search import TavilySearchResults
    web_search_tool = TavilySearchResults(k=3)
    return (web_search_tool,)


@app.cell
def _(Annotated, List, TypedDict):
    ### State
    import operator

    class GraphState(TypedDict):
        """
        Graph state is a dictionary that contains information we want to propagate to, and modify in, each graph node.
        """

        question: str  # User question
        generation: str  # LLM generation
        web_search: str  # Binary decision to run web search
        max_retries: int  # Max number of retries for answer generation
        answers: int  # Number of answers generated
        loop_step: Annotated[int, operator.add]
        documents: List[str]  # List of retrieved documents
    return (GraphState,)


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    We'll implement these as a control flow in LangGraph.
    """)
    return


@app.cell
def _(
    GradeAnswer,
    GradeDocuments,
    GradeHallucinations,
    HumanMessage,
    RouteQuery,
    SystemMessage,
    answer_grader_instructions,
    answer_grader_prompt,
    doc_grader_instructions,
    doc_grader_prompt,
    format_docs,
    hallucination_grader_instructions,
    hallucination_grader_prompt,
    llm,
    rag_prompt,
    retriever,
    router_instructions,
    web_search_tool,
):
    from langchain.schema import Document
    from langgraph.graph import END

    ### Nodes
    def retrieve(state):
        """
        Retrieve documents from vectorstore

        Args:
            state (dict): The current graph state

        Returns:
            state (dict): New key added to state, documents, that contains retrieved documents
        """
        print('---RETRIEVE---')
        question = state['question']
        documents = retriever.invoke(question)
        return {'documents': documents}  # Write retrieved documents to documents key in state

    def generate(state):
        """
        Generate answer using RAG on retrieved documents

        Args:
            state (dict): The current graph state

        Returns:
            state (dict): New key added to state, generation, that contains LLM generation
        """
        print('---GENERATE---')
        question = state['question']
        documents = state['documents']
        loop_step = state.get('loop_step', 0)
        docs_txt = format_docs(documents)
        rag_prompt_formatted = rag_prompt.format(context=docs_txt, question=question)
        generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])
        return {'generation': generation, 'loop_step': loop_step + 1}  # RAG generation

    def grade_documents(state):
        """
        Determines whether the retrieved documents are relevant to the question
        If any document is not relevant, we will set a flag to run web search

        Args:
            state (dict): The current graph state

        Returns:
            state (dict): Filtered out irrelevant documents and updated web_search state
        """
        print('---CHECK DOCUMENT RELEVANCE TO QUESTION---')
        question = state['question']
        documents = state['documents']
        filtered_docs = []
        web_search = 'No'
        _structured_llm_grader = llm.with_structured_output(GradeDocuments)
        for d in documents:
            doc_grader_prompt_formatted = doc_grader_prompt.format(document=d.page_content, question=question)
            _result = _structured_llm_grader.invoke([SystemMessage(content=doc_grader_instructions)] + [HumanMessage(content=doc_grader_prompt_formatted)])
            grade = _result.binary_score  # Score each doc
            if grade.lower() == 'yes':
                print('---GRADE: DOCUMENT RELEVANT---')
                filtered_docs.append(d)
            else:
                print('---GRADE: DOCUMENT NOT RELEVANT---')
                web_search = 'Yes'
                continue
        return {'documents': filtered_docs, 'web_search': web_search}

    def web_search(state):
        """
        Web search based based on the question

        Args:  # Document relevant
            state (dict): The current graph state

        Returns:
            state (dict): Appended web results to documents  # Document not relevant
        """
        print('---WEB SEARCH---')
        question = state['question']  # We do not include the document in filtered_docs
        documents = state.get('documents', [])  # We set a flag to indicate that we want to run web search
        _docs = web_search_tool.invoke({'query': question})
        web_results = '\n'.join([d['content'] for d in _docs])
        web_results = Document(page_content=web_results)
        documents.append(web_results)
        return {'documents': documents}

    def route_question(state):
        """
        Route question to web search or RAG

        Args:
            state (dict): The current graph state

        Returns:
            str: Next node to call
        """
        print('---ROUTE QUESTION---')
        structured_llm_router = llm.with_structured_output(RouteQuery)
        route_question = structured_llm_router.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=state['question'])])
        source = route_question.datasource
        if source == 'websearch':  # Web search
            print('---ROUTE QUESTION TO WEB SEARCH---')
            return 'websearch'
        elif source == 'vectorstore':
            print('---ROUTE QUESTION TO RAG---')
            return 'vectorstore'

    ### Edges
    def decide_to_generate(state):
        """
        Determines whether to generate an answer, or add web search

        Args:
            state (dict): The current graph state

        Returns:
            str: Binary decision for next node to call
        """
        print('---ASSESS GRADED DOCUMENTS---')
        question = state['question']
        web_search = state['web_search']
        filtered_documents = state['documents']
        if web_search == 'Yes':
            print('---DECISION: NOT ALL DOCUMENTS ARE RELEVANT TO QUESTION, INCLUDE WEB SEARCH---')
            return 'websearch'
        else:
            print('---DECISION: GENERATE---')
            return 'generate'

    def grade_generation_v_documents_and_question(state):
        """
        Determines whether the generation is grounded in the document and answers question

        Args:
            state (dict): The current graph state

        Returns:
            str: Decision for next node to call
        """
        print('---CHECK HALLUCINATIONS---')
        question = state['question']
        documents = state['documents']
        generation = state['generation']
        max_retries = state.get('max_retries', 3)
        hallucination_grader_prompt_formatted = hallucination_grader_prompt.format(documents=format_docs(documents), generation=generation.content)
        _structured_llm_grader = llm.with_structured_output(GradeHallucinations)
        _result = _structured_llm_grader.invoke([SystemMessage(content=hallucination_grader_instructions)] + [HumanMessage(content=hallucination_grader_prompt_formatted)])
        grade = _result.binary_score
        if grade == 'yes':
            print('---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---')
            print('---GRADE GENERATION vs QUESTION---')
            answer_grader_prompt_formatted = answer_grader_prompt.format(question=question, generation=generation.content)  # All documents have been filtered check_relevance
            _structured_llm_grader = llm.with_structured_output(GradeAnswer)  # We will re-generate a new query
            _result = _structured_llm_grader.invoke([SystemMessage(content=answer_grader_instructions)] + [HumanMessage(content=answer_grader_prompt_formatted)])
            grade = _result.binary_score
            if grade == 'yes':
                print('---DECISION: GENERATION ADDRESSES QUESTION---')
                return 'useful'
            elif state['loop_step'] <= max_retries:  # We have relevant documents, so generate answer
                print('---DECISION: GENERATION DOES NOT ADDRESS QUESTION---')
                return 'not useful'
            else:
                print('---DECISION: MAX RETRIES REACHED---')
                return 'max retries'
        elif state['loop_step'] <= max_retries:
            print('---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---')
            return 'not supported'
        else:
            print('---DECISION: MAX RETRIES REACHED---')
            return 'max retries'  # Default to 3 if not provided  # Check hallucination  # Check question-answering  # Test using question and generation from above  # Grade answer
    return (
        END,
        decide_to_generate,
        generate,
        grade_documents,
        grade_generation_v_documents_and_question,
        retrieve,
        route_question,
        web_search,
    )


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    ### Graph Build
    """)
    return


@app.cell
def _(
    END,
    GraphState,
    decide_to_generate,
    generate,
    grade_documents,
    grade_generation_v_documents_and_question,
    retrieve,
    route_question,
    web_search,
):
    from langgraph.graph import StateGraph
    from IPython.display import Image, display

    workflow = StateGraph(GraphState)

    # Define the nodes
    workflow.add_node("websearch", web_search)  # web search
    workflow.add_node("retrieve", retrieve)  # retrieve
    workflow.add_node("grade_documents", grade_documents)  # grade documents
    workflow.add_node("generate", generate)  # generate

    # Build graph
    workflow.set_conditional_entry_point(
        route_question,
        {
            "websearch": "websearch",
            "vectorstore": "retrieve",
        },
    )
    workflow.add_edge("websearch", "generate")
    workflow.add_edge("retrieve", "grade_documents")
    workflow.add_conditional_edges(
        "grade_documents",
        decide_to_generate,
        {
            "websearch": "websearch",
            "generate": "generate",
        },
    )
    workflow.add_conditional_edges(
        "generate",
        grade_generation_v_documents_and_question,
        {
            "not supported": "generate",
            "useful": END,
            "not useful": "websearch",
            "max retries": END,
        },
    )

    # Compile
    graph = workflow.compile()
    display(Image(graph.get_graph().draw_mermaid_png()))
    return (graph,)


@app.cell
def _(graph):
    # Test on vectorstore
    _inputs = {'question': 'What are the types of agent memory?', 'max_retries': 3}
    for _event in graph.stream(_inputs, stream_mode='values'):
        print(_event)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    Trace:

    https://smith.langchain.com/public/924df598-4666-4620-bce1-6b3d74313451/r
    """)
    return


@app.cell
def _(graph):
    # Test on current events
    _inputs = {'question': 'What are the most recent llama3 models released?', 'max_retries': 3}
    for _event in graph.stream(_inputs, stream_mode='values'):
        print(_event)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
    Trace:

    https://smith.langchain.com/public/d8935288-6597-4357-b92e-3e64baff57c2/r
    """)
    return


@app.cell(hide_code=True)
def _(mo):
    mo.md(r"""
 
    """)
    return


@app.cell
def _():
    import marimo as mo
    return (mo,)


if __name__ == "__main__":
    app.run()
